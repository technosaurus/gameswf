Chunk LOD demo
1 Jan 2002
Thatcher Ulrich <tu@tulrich.com> http://tulrich.com

Demo of chunked LOD scheme.  Uses static meshes with lerping and
crack-filling meshes to give an adjustable guaranteed max pixel error,
no vertex popping and hopefully low CPU overhead.

"chunkdemo" is the renderer.  "heightfield_chunker" is the tool that
precomputes the chunk data.  "heightfield_shader" is a simple tool for
generating a big texture to stretch over a heightfield.  All programs
print usage info when you run them.

With vertex_array_range extension, the demo puts out up to 10 million
tris/sec on my laptop (1GHz CPU, GeForce2 Go video) under optimal
conditions.  NVIDIA's VAR demo does ~11 Mtris/s on this machine, so
I'm doing pretty well.  However, the throughput is typically much
lower, depending on framerate, number of chunks, etc.  4M tris/sec is
a typical throughput on my machine, with a really big dataset and good
(30+ Hz) framerate.

The strips currently generated by the chunker have a lot of degenerate
tris in them; typically more than 0.5x the number of real triangles,
so I think throughput improvement is possible.  I'm just using the
automatic binary-triangle-tree strip generation approach from the
Lindstrom et al SIGGRAPH '96 paper.  A stripifier designed for
arbitrary meshes might be able to do better.

Crater dataset thanks to John Ratcliff, http://ratcliff.flipcode.com

Thanks to Ben Discoe & http://www.vterrain.org for the .bt terrain
file format and for providing some sample .bt data.


USAGE

To view the included crater dataset, go into demos/chunklod and run
"chunkdemo".  By default it will look for "crater/crater.chu" and
"crater/crater.jpg", and print some usage info to the console.

To experiment with other datasets, you need to generate a .chu file
from heightfield data, using the heightfield_chunker program.  If you
run "heightfield_chunker" with no arguments, it will print a usage
summary.  For heightfield data input, you have two options.  You can
use the .BT format, which is a nice convenient format for
high-precision terrain data, or you can use a plain old bitmap, and
encode the height information according to the brightness of each
pixel.  Unfortunately the heightfield_chunker can currently only
extract 8 bits/pixel of height data from a bitmap.  The chunker can
read many types of bitmap files, including .jpg.

Run the chunker with "heightfield_chunker input.bt output.chu".  The
program picks some default values for the base error ("-e error") and
tree depth ("-d depth").  Watch the output summary, to see if you
might want to change the default values.  In general, a larger base
error value will generate smaller, less detailed chunk files.  For
tree depth, a higher value is better for bigger datasets, but too high
a value will make chunks without enough vertices in them, and the
overall overhead will be larger.  You probably want an average
vertex/chunk value of somewhere between 1000 and 5000.

You can get some nice sample .BT data at:
http://vterrain.org/BT/index.html .  If you don't have a texture for
your dataset, you can make a basic one using "heightfield_shader".
Example: "heightfield_shader heightfield.bt output_texture.bmp".
There are various options, including a crude terrain-type facility;
run "heightfield_shader" with no args to see a usage summary.

To view the heightfield, use "chunkdemo chunkdata.chu texture.bmp".
The program will open a rendering window, and print usage info to the
console.


CHANGES

1 Mar 2002

Egads!  I was going over the LOD formulas, and they're off by a factor
of 2!  The pixel errors previously reported were too pessimistic!
This means two things: 1) I'm a dope for missing this, and 2) the demo
actually works "better" than previously reported.  Although, now I
discover that using 5 as a max-pixel-error can introduce pops when LOD
jumps completely over a level.  I'll have to ponder the correct
solution to this problem -- perhaps a restricted time-based morph.


Sometime in Jan/Feb 2002

Memory-mapping utilities, for processing giant .bt's, and future
paging experiments.  I'm going to try a different approach though, for
the Puget Sound dataset: break into 16 tiles of 4k x 4k each.  I'll
mmap the .bt image on disk for the heightfield_chunker, to avoid the
long useless loading phase.

GL_SHORTs are horribly slow for NVIDIA's GeForce2Go driver to process
--> back to floats.  Will have to switch over to using a vertex shader
before that type of compression becomes feasible.

Added some prerequisites to paging.  Actual chunk data is loaded on
demand.  Currently it's never paged out though.


1 Jan 2002

I'm passing GL_SHORTs to glVertexPointer, using glTranslatef and
glScalef to do the simple decompression, instead of doing it via CPU.
Lightens the CPU load, and cuts the size/bandwidth of AGP vertex data
in half.

Also, I rearranged the edge data so that the edge data is not shared;
instead each chunk is packaged together with edge data for all four of
the chunk's borders.  This causes a little data redundancy, but it
lets the edge verts be quantized relative to the chunk's local
coordinates, and simplifies the renderer slightly.


17 Nov 2001

Quantization of vertices to 16 bits/coord.

Stripped meshes, instead of triangle lists -- much improved throughput
on NVidia GPU's, and smaller chunk files.

Top-down LOD update -- algo is output-sensitive now; LOD update takes
negligible time, even for large datasets with deep trees.

Edge meshes are generated by the chunker, correctly.  Before they were
generated at load time by the renderer, which occasionally caused
funky edge meshes.

Many bug fixes and optimizations.  Large datasets can now be processed
and viewed.

Quickie heightfield shader program, for generating a statically-lit
synthetic texture for a heightfield.

With the changes, I see bytes/heixel ratios of ~8 or less, for typical
big natural .BT terrain files with base error of 1m.


ALGORITHM NOTES

The basic idea is to take a single object, and make a tree of chunks
out of it.  Each chunk is just a static, precomputed mesh that can be
rendered with a single glDrawElements() call.  The chunk at the root
of the tree is a low-detail representation of the entire object.  The
child chunks of the root node split the object into several pieces,
and each piece independently represents its portion of the object,
with a higher level of detail than the parent.  This splitting happens
recursively down to some arbitrary depth.  Each chunk has a bounding
volume associated with it, and each level of the tree has a maximum
geometric error associated with the chunks at that level.  The max
geometric error represents maximum deviation of a chunk mesh from the
underlying object geometry it represents.  In my scheme the max
geometric error reduces by a factor of 2 at every additional level in
the tree.  For example, if the root node of the tree is a single mesh
that represents the object with at most 16 units of deviation from the
full-detail mesh, at the fifth level down the tree, the chunks each
represent a small piece of the object, with only 1 unit of deviation.

The renderer simply chooses which chunks of the tree to draw, by
looking at the viewpoint location in relation to the field-of-view,
max allowed pixel error, and max geometric error of the chunks.  This
tree is very similar in concept to Hoppe's Progressive Meshes; just
using whole meshes instead of individual vertices.

There are two main problems with the above algorithm:

1) When switching LODs, the meshes will pop suddenly.

2) The chunks at different LODs will have cracks where they meet.

I've solved both problems.  1) is taken care of by lerping the meshes.
Given a chunk A, and one of its child chunks B, I require that A
contain some subset of the vertices of B (no sweat), call them L.
These vertices are not lerped when rendering B.  Let L' be the verts
in B which *are not* contained in A.  The L' verts *are* lerped; when
A first transitions to B, the L' verts are positioned so that they are
coincident with points on the A mesh.  These positions can be on a
vertex, edge or face; it doesn't matter much as long as they're on the
A mesh, somewhere close to their final position.  The L' verts are
gradually lerped towards the final position as B gets closer to the
viewpoint.  By the time B is subdivided further, all the vertices in B
are in their final positions.

This causes an LOD transition to be completely invisible.  The
artifacts of adding vertices are hidden by the gradual morphing of
vertices, which tends to be visually unobtrusive.

It does occasionally happen in my demo that an LOD transition will
cause a pop.  This bears further investigation; I think this may be
related to the way I compute the distance from the viewpoint to the
nearest point in a chunk.  However, it could be an inherent flaw in
the approach.  Nevertheless, the practical visual impact is minimal.

Problem 2), cracks between chunks, is made even worse by the lerping
scheme.  Adjacent chunks at the same level are not guaranteed to match
up exactly at the edge, because the chunks may be in different stages
of lerping.  However, in both this case, and in the case where chunks
at different LODs meet, the cracks can be filled with special edge
meshes.  These are just simple meshes that form a "ribbon" of
geometry, stitching together the two edges.  These ribbons are subject
to lerping just like the chunks, although there are two or three lerp
parameters per edge ribbon, rather than just one as in the chunks.
However, the total number of triangles in edge ribbons is small
compared to the whole mesh, and it's very effective at filling edge
cracks.  This crack-filling scheme means that there are no T-junctions
anywhere in the mesh.

Texturing for this LOD scheme is actually extremely simple, given an
additional contraint: if the maximum pixel error is fixed at
preprocessing time, the chunk preprocessor can straightforwardly
generate a unique static texture for each chunk.  The necessary
resolution of the texture can be pre-computed using the LOD math, so
that the precomputed texture provides maximum visible detail without
wasting RAM.  Because the chunks are localized, it should also be
possible to compute a decent per-chunk texgen mapping so that no
vertex u,v's need to be stored in most cases.

There are some interesting open questions about texture popping
though, if the mappings change significantly between chunks.  My sense
is that if enough texture resolution is available, which should be
easy to guarantee, that the popping would be minimal, but I haven't
tried it yet.  Of course the sensible thing to do is probably to just
hardcode texture u,v's in the vertex data.  It's a shameful waste of
memory most of the time, but it's flexible, very common, and with
paging & adequate buffers it shouldn't hurt too much.

In this demo, though, I'm just using a simple large projected texture
over the whole model.

This demo also only works with heightfields.  I did that because
heightfields are a little easier to deal with, and I knew how to do
the mesh reduction.  The chunker is very specific to heightfields,
while the renderer is only partly specific to heightfields and can
easily be made generic.  This whole chunked-LOD scheme should be
applicable to general meshes.  You'd just need to chunk up the meshes
in some spatially-coherent way; clipping a general mesh with a octree
would be one straightforward approach, or taking a Progressive Mesh
split tree and picking out certain subtrees to be the chunks would
probably work even better.

Mesh Compression

One interesting property of the chunked LOD tree is that the vertices
in an individual chunks are spatially-localized.  The chunks vary in
size in world coordinates, but generally stay within some restricted
bounds in *screen* coordinates.  So, if the vertices in a chunk are
compressed using a simple quantization scheme, the quantization errors
will be bounded in screen space.  This is a very helpful property,
since the size overhead of chunked LOD vs. an ordinary mesh can be
substantial.  In the current demo I've only quantized down to 16 bits
per coordinate.  I'm curious to see whether 8 bits/coord gives
acceptable results (perhaps it won't).
